{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "uXSCtJQpRpb0"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Split audio by silence\n",
        "\n",
        "This script processes a folder of audio files,\n",
        "detects silence-based segments in each audio,**\n",
        "matches each segment to corresponding transcript text\n",
        "from a JSON file (e.g. AWS Transcribe),\n",
        "and exports the results as segmented MP3s + a CSV.\n",
        "\n",
        "Useful for turning long recordings into timestamped snippets."
      ],
      "metadata": {
        "id": "0UAL9oMD0aK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to Google Drive"
      ],
      "metadata": {
        "id": "9zQcaLK2JeL1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Qe1OWMh40LRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65345dde-f7e9-458c-fbdf-97ffd97be90d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Required Packages"
      ],
      "metadata": {
        "id": "uXSCtJQpRpb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y ffmpeg\n",
        "!apt-get install tree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCSk1287RzfF",
        "outputId": "4603db3f-8992-44f5-f24d-5ec1b0020313"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 47.9 kB of archives.\n",
            "After this operation, 116 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
            "Fetched 47.9 kB in 1s (73.2 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 126281 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries"
      ],
      "metadata": {
        "id": "OFqIJ8wa0Sj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm  # For progress bars\n",
        "from os.path import join\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence  # Detects silent gaps in audio"
      ],
      "metadata": {
        "id": "lhj3r2Wi0V9a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore Data structure"
      ],
      "metadata": {
        "id": "I3eJaCTmS9q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tree '/content/drive/MyDrive/a_tutorial_notebooks/data'"
      ],
      "metadata": {
        "id": "grM4LIZ9BPw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaf837e0-6c4d-474b-a639-2a51d344624a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34m/content/drive/MyDrive/a_tutorial_notebooks/data\u001b[0m\n",
            "└── \u001b[01;34m1\u001b[0m\n",
            "    ├── \u001b[01;34mprocessed\u001b[0m\n",
            "    │   ├── \u001b[01;35m2c9f9799-b379-4c6e-91dd-38a1897d4ff6.mp3\u001b[0m\n",
            "    │   ├── \u001b[00m2c9f9799-b379-4c6e-91dd-38a1897d4ff6.mp4\u001b[0m\n",
            "    │   └── \u001b[00mthumbnail.png\u001b[0m\n",
            "    └── \u001b[01;34mtranscription\u001b[0m\n",
            "        └── \u001b[00mtranscribe_output.json\u001b[0m\n",
            "\n",
            "3 directories, 4 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Splits the input audio file into segments based on silence."
      ],
      "metadata": {
        "id": "9HTt5jSB0tsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Function: split_audio_by_silence_segments\n",
        "# -------------------------------------------\n",
        "def split_audio_by_silence_segments(input_file, silence_threshold=-50, min_silence_duration=200):\n",
        "    \"\"\"\n",
        "    Splits the input audio file into segments based on silence.\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to the audio file (e.g., .mp3 or .wav)\n",
        "        silence_threshold (int): Volume in dBFS below which is considered silence\n",
        "        min_silence_duration (int): Minimum silence length (in ms) to split on\n",
        "\n",
        "    Returns:\n",
        "        list of AudioSegment: Each chunk is a speech segment between silent parts\n",
        "    \"\"\"\n",
        "    audio = AudioSegment.from_file(input_file)\n",
        "\n",
        "    print(f\"Loaded audio with {audio.frame_count()} frames.\")\n",
        "\n",
        "    # Automatically split audio where silence is detected\n",
        "    segments = split_on_silence(\n",
        "        audio,\n",
        "        min_silence_len=min_silence_duration,\n",
        "        silence_thresh=silence_threshold,\n",
        "        keep_silence=True  # Adds padding so transitions aren't too abrupt\n",
        "    )\n",
        "    return segments"
      ],
      "metadata": {
        "id": "mi_BnIpw07Sw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculates the start and end times (in seconds) of each segment.\n"
      ],
      "metadata": {
        "id": "yyEoOcaX2eAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Function: segment_times\n",
        "# -------------------------------------------\n",
        "def segment_times(segments):\n",
        "    \"\"\"\n",
        "    Calculates the start and end times (in seconds) of each segment.\n",
        "\n",
        "    Args:\n",
        "        segments (list of AudioSegment): Segments from silence splitting\n",
        "\n",
        "    Returns:\n",
        "        list of tuples: Each tuple is (start_time, end_time) of a segment\n",
        "    \"\"\"\n",
        "    start_time = 0\n",
        "    timestamps = []\n",
        "\n",
        "    for segment in segments:\n",
        "        # Duration in seconds = total frames / frame rate\n",
        "        length = segment.frame_count() / segment.frame_rate\n",
        "        end_time = start_time + length\n",
        "        timestamps.append((start_time, end_time))\n",
        "        start_time = end_time  # Start next segment where this one ends\n",
        "\n",
        "    return timestamps\n"
      ],
      "metadata": {
        "id": "pzShbrSn01E2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save each audio segment as a separate .mp3 file."
      ],
      "metadata": {
        "id": "h88_a1Qa2jBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Function: export_chunks\n",
        "# -------------------------------------------\n",
        "def export_chunks(chunk_path, segments):\n",
        "    \"\"\"\n",
        "    Saves each audio segment as a separate .mp3 file.\n",
        "\n",
        "    Args:\n",
        "        chunk_path (str): Output directory for chunks\n",
        "        segments (list of AudioSegment): List of split audio segments\n",
        "    \"\"\"\n",
        "    os.makedirs(chunk_path, exist_ok=True)\n",
        "\n",
        "    for i, segment in enumerate(segments):\n",
        "        output_file = os.path.join(chunk_path, f\"chunk_{i}.mp3\")\n",
        "        segment.export(output_file, format=\"mp3\")  # Save in mp3 format"
      ],
      "metadata": {
        "id": "visZ7ZSo05vz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converts a transcription JSON file to a DataFrame."
      ],
      "metadata": {
        "id": "dUpSjk112l7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# Function: json_to_df\n",
        "# -------------------------------------------\n",
        "def json_to_df(input_file):\n",
        "    \"\"\"\n",
        "    Converts a transcription JSON file to a DataFrame.\n",
        "\n",
        "    Expects JSON in AWS Transcribe format:\n",
        "    - Each word has a start_time and end_time.\n",
        "    - Punctuation marks have no time, so we append them to the last word.\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to the transcription JSON file\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Columns are 'stime', 'etime', and 'word'\n",
        "    \"\"\"\n",
        "    with open(input_file) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    start_time, end_time, word = [], [], []\n",
        "\n",
        "    for item in data['results']['items']:\n",
        "        if \"start_time\" in item:\n",
        "            start_time.append(float(item['start_time']))\n",
        "            end_time.append(float(item['end_time']))\n",
        "            word.append(item['alternatives'][0]['content'])\n",
        "        else:\n",
        "            # If it's punctuation (no timing), append it to the previous word\n",
        "            word[-1] += item['alternatives'][0]['content']\n",
        "\n",
        "    return pd.DataFrame({'stime': start_time, 'etime': end_time, 'word': word})\n",
        "\n",
        "# -------------------------------------------"
      ],
      "metadata": {
        "id": "9XgPIrLU1EO5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Match word timestamps to audio segments, generating aligned transcript snippets.\n"
      ],
      "metadata": {
        "id": "8YXI4Uvf2pg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function: final_df\n",
        "# -------------------------------------------\n",
        "def final_df(timestamps, word_df):\n",
        "    \"\"\"\n",
        "    Matches word timestamps to audio segments, generating aligned transcript snippets.\n",
        "\n",
        "    Args:\n",
        "        timestamps (list): List of (start_time, end_time) tuples for each audio chunk\n",
        "        word_df (pd.DataFrame): Word-level transcript with start/end times\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: One row per audio chunk, with aligned sentence and timing\n",
        "    \"\"\"\n",
        "    start_time, end_time, sentence, chunk = [], [], [], []\n",
        "\n",
        "    for idx, (start, end) in enumerate(timestamps):\n",
        "        # Filter words that fall within the current segment\n",
        "        temp_df = word_df[(word_df['stime'] >= start) & (word_df['etime'] <= end)]\n",
        "        text = ' '.join(temp_df['word'].tolist())\n",
        "\n",
        "        if len(text.strip()) > 0:\n",
        "            start_time.append(start)\n",
        "            end_time.append(end)\n",
        "            sentence.append(text)\n",
        "            chunk.append(idx)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'start_time': start_time,\n",
        "        'end_time': end_time,\n",
        "        'sentence': sentence,\n",
        "        'chunk': chunk\n",
        "    })"
      ],
      "metadata": {
        "id": "va_i1D9D1Epd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Script: Process All Audio Files in Folder"
      ],
      "metadata": {
        "id": "eytnY0rF2s7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------------------------\n",
        "# Main Script: Process All Audio Files in Folder\n",
        "# -------------------------------------------\n",
        "\n",
        "# 🛣️ Define root path\n",
        "data_root = \"/content/drive/MyDrive/a_tutorial_notebooks/data\"\n",
        "\n",
        "\n",
        "# Path to folder containing subfolders (named numerically) with audio and transcript data\n",
        "csv_files = data_root\n",
        "csv_root = data_root\n",
        "csv_path = \"media/\"  # Subfolder containing .mp3 files\n",
        "\n",
        "# Loop through all folders in the root\n",
        "for folder in tqdm(os.listdir(csv_files), desc=\"Processing folders\"):\n",
        "    if folder.isdigit():  # Process only numeric folders\n",
        "        root_folder = os.path.join(csv_root, folder)\n",
        "        mp3_folder = os.path.join(root_folder, csv_path)\n",
        "        chunk_path = os.path.join(root_folder, \"audio_clips\")\n",
        "        json_file = os.path.join(root_folder, \"transcription\", \"transcribe_output.json\")\n",
        "        output_csv = os.path.join(root_folder, f\"{folder}_splits.csv\")\n",
        "\n",
        "        for file in os.listdir(mp3_folder):\n",
        "            if file.endswith(\".mp3\"):\n",
        "                input_audio = os.path.join(mp3_folder, file)\n",
        "\n",
        "                # Load transcript, split audio, and export results\n",
        "                transcript_df = json_to_df(json_file)\n",
        "                segments = split_audio_by_silence_segments(input_audio)\n",
        "                export_chunks(chunk_path, segments)\n",
        "                timestamps = segment_times(segments)\n",
        "                final_transcript_df = final_df(timestamps, transcript_df)\n",
        "                final_transcript_df.rename(columns={\"chunks\": \"segments\"}, inplace=True)\n",
        "\n",
        "                # Save timestamped sentence segments to CSV\n",
        "                final_transcript_df.to_csv(output_csv, index=False)\n"
      ],
      "metadata": {
        "id": "3QvH3zDL1Ho2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c2e19d2-9b95-486a-9202-949bf7e62a1d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing folders:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded audio with 89839200.0 frames.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing folders: 100%|██████████| 1/1 [04:24<00:00, 264.35s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CJXkeSVR2wBi"
      }
    }
  ]
}